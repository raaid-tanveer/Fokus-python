{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597424278766",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/raaidtanveer/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nWarming up PyWSD (takes ~10 secs)...took 12.80709719657898 secs.\n"
    }
   ],
   "source": [
    "#importing libraries\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import urllib.parse\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import re\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching youtube video transcript\n",
    "url = \"https://www.youtube.com/watch?v=2ujmYuSYzSk\"\n",
    "url_data = urllib.parse.urlparse(url)\n",
    "query = urllib.parse.parse_qs(url_data.query)\n",
    "video_id = query[\"v\"][0]\n",
    "transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "text = \"\"\n",
    "for d in transcript:\n",
    "    txt = d['text'].replace('\\n', ' ')\n",
    "    text += txt + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate tfidf vectors for words in the text excluding stopwords. Takes only text parameter. Returns a pandas dataframe with the words and their respective tfidf vector values.\n",
    "def generate_tfidf_vectors(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame({'word':feature_names,'value':denselist[0]})\n",
    "    sorted_df = df.sort_values(by=['value'], ascending=False)\n",
    "    final_df = sorted_df[~sorted_df['word'].isin(stopwords.words('english'))]\n",
    "    final_df.reset_index(inplace=True,drop=True)\n",
    "    return final_df\n",
    "#function to return a dictionary of format {word:[sentences]}. Takes text and word to search as parameters. \n",
    "def _find_sentences_with_words(raw_text, words):\n",
    "    sentences = []\n",
    "    for w in words:\n",
    "        text = re.split('(?<=[.!?])',raw_text)\n",
    "        neo_sentences = []\n",
    "        for sent in text:\n",
    "            if w in sent:\n",
    "                neo_sentences.append(sent)\n",
    "        sentences.append(neo_sentences)\n",
    "    dictionary = dict.fromkeys(words, sentences)\n",
    "    for key in dictionary:\n",
    "        flat_list = []\n",
    "        for sublist in dictionary[key]:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "        dictionary[key] = flat_list\n",
    "    return dictionary\n",
    "    \n",
    "d = _find_sentences_with_words(text, generate_tfidf_vectors(text)['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distractors from Wordnet\n",
    "def get_distractors_wordnet(syn,word):\n",
    "    distractors=[]\n",
    "    word= word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms()\n",
    "    if len(hypernym) == 0: \n",
    "        return distractors\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        #print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\",\" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors\n",
    "\n",
    "def get_wordsense(sent,word):\n",
    "    word= word.lower()\n",
    "    \n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    synsets = wn.synsets(word,'n')\n",
    "    if synsets:\n",
    "        #wup = max_similarity(sent, word, 'wup', pos='n')\n",
    "        #adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
    "        #lowest_index = min(synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "        return synsets[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Distractors from http://conceptnet.io/\n",
    "def get_distractors_conceptnet(word):\n",
    "    word = word.lower()\n",
    "    original_word= word\n",
    "    if (len(word.split())>0):\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    distractor_list = [] \n",
    "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
    "    obj = requests.get(url).json()\n",
    "\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term'] \n",
    "\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "                distractor_list.append(word2)\n",
    "                   \n",
    "    return distractor_list\n",
    "#getting a dictionary of format {keyword:[distractors]}\n",
    "key_distractor_list = {}\n",
    "for keyword in d:\n",
    "    wordsence = get_wordsense\n",
    "for keyword in d:\n",
    "    wordsense = get_wordsense(d[keyword][0], keyword)\n",
    "    if wordsense:\n",
    "        distractors = get_distractors_wordnet(wordsense,keyword)\n",
    "        if len(distractors) ==0:\n",
    "            distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "    else:\n",
    "        distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#finds a list of sentences from raw_text containing a particular word. Function takes raw text and word to look for as parameters. Returns a list of sentences containing word.\n",
    "def _find_sentences_with_word(raw_text, word):\n",
    "    text = re.split('(?<=[.!?])',raw_text)\n",
    "    neo_sentences = []\n",
    "    for sent in text:\n",
    "        if word in sent:\n",
    "            neo_sentences.append(sent)\n",
    "    return neo_sentences\n",
    "\n",
    "#creating distractor words\n",
    "def _create_distractor(text, word):\n",
    "    replace_word = \"\"\n",
    "    for w in key_distractor_list[word]:\n",
    "        if (w.lower() != word.lower()):\n",
    "            replace_word = w\n",
    "            break\n",
    "    tf = round(np.random.random())\n",
    "    if tf==0:\n",
    "        answer = True\n",
    "        replace_word = word.lower()\n",
    "    else:\n",
    "        answer = False\n",
    "    return (text.replace(word, replace_word.lower()), answer)\n",
    "\n",
    "#creates a ranked pandas dataframe of questions. Ranks based on the average tfidf vector value per word multiplied by the tfidf vector value for the substituted word. Takes the text and search depth as parameter, search depth being the number of words with the highest tfidf vector values to use to generate questions. Returns a ranked dataframe of questions with their respective answers, average tfidf vector values per word in the sentence and scores(average tfidf vector value per word multiplied by the tfidf vector value for the substituted word). \n",
    "def ranked_question_df(text):\n",
    "    sentences = pd.DataFrame(columns=['sentences','avg_tfidf_per_word', 'answer', 'score'])\n",
    "    tfidf_df = generate_tfidf_vectors(text)\n",
    "    for word in key_distractor_list:\n",
    "        for sentence in _find_sentences_with_word(text, word):\n",
    "            words = set(sentence.split(' '))\n",
    "            avg = tfidf_df[tfidf_df['word'].isin(words)]['value'].sum()/len(words)\n",
    "            sentences = sentences.append(pd.Series({'sentences':_create_distractor(sentence, word)[0], 'avg_tfidf_per_word':avg, 'answer':_create_distractor(sentence, word)[1], 'score': avg * float(tfidf_df[tfidf_df['word']==word]['value'])}), ignore_index=True)\n",
    "    return sentences.drop_duplicates().sort_values(by=['score'], ascending=False)\n",
    "\n",
    "questions = ranked_question_df(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                             sentences  avg_tfidf_per_word  \\\n64    I think a big one here would be kind of an in...            0.015525   \n130                       I really like this memorial.            0.015490   \n194   I think a big binary digit here would be kind...            0.015525   \n89    And then the tone I would say is pigeonhole o...            0.012543   \n28    All absolute, so that is one short answer que...            0.011988   \n\n    answer     score  \n64   False  0.001087  \n130   True  0.000939  \n194   True  0.000899  \n89   False  0.000879  \n28   False  0.000856  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentences</th>\n      <th>avg_tfidf_per_word</th>\n      <th>answer</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>64</th>\n      <td>I think a big one here would be kind of an in...</td>\n      <td>0.015525</td>\n      <td>False</td>\n      <td>0.001087</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>I really like this memorial.</td>\n      <td>0.015490</td>\n      <td>True</td>\n      <td>0.000939</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>I think a big binary digit here would be kind...</td>\n      <td>0.015525</td>\n      <td>True</td>\n      <td>0.000899</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>And then the tone I would say is pigeonhole o...</td>\n      <td>0.012543</td>\n      <td>False</td>\n      <td>0.000879</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>All absolute, so that is one short answer que...</td>\n      <td>0.011988</td>\n      <td>False</td>\n      <td>0.000856</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{' I think a big one here would be kind of an increase in anti-immigrant sentiment or nativist sentiment.': False,\n ' I really like this memorial.': True,\n ' I think a big binary digit here would be kind of an increase in anti-immigrant sentiment or nativist sentiment.': True,\n \" And then the tone I would say is pigeonhole of somber, respectful, it's even pigeonhole of quiet.\": False,\n ' All absolute, so that is one short answer question.': False}"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dict(zip(questions.iloc[:5]['sentences'].tolist(),questions.iloc[:5]['answer'].tolist() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}